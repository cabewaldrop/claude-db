{"id":"cdb-0gq","title":"Secondary index support (CREATE INDEX)","description":"Add support for creating and using secondary indexes on non-PK columns.\n\n## Current State\n- Only primary key has an index\n- README lists 'Indexes beyond primary key' as a limitation\n\n## Implementation\n1. Add CREATE INDEX statement to parser\n2. Extend Table struct with indexes map[string]*BTree\n3. Maintain secondary indexes on INSERT/UPDATE/DELETE\n4. Query planner selects best index for WHERE clause\n\n## Syntax\n```sql\nCREATE INDEX users_age ON users(age);\nSELECT * FROM users WHERE age \u003e 30;  -- Uses index\n```\n\n## Files\n- internal/sql/parser/parser.go (new statement type)\n- internal/sql/ast/ast.go (CreateIndexStatement)\n- internal/table/table.go (multiple index support)\n- internal/sql/executor/executor.go (CREATE INDEX execution)\n- internal/sql/planner/ (index selection)\n\n## Acceptance Criteria\n- CREATE INDEX syntax works\n- Secondary indexes maintained on writes\n- Planner can choose secondary index for queries","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-07T12:19:05.789451-07:00","created_by":"mayor","updated_at":"2026-01-07T14:05:19.910546-07:00","closed_at":"2026-01-07T12:21:32.935096-07:00","close_reason":"Superseded by existing claude-db-xsr (Support secondary indexes) which has full implementation plan including catalog changes, DML maintenance, and persistence","dependencies":[{"issue_id":"cdb-0gq","depends_on_id":"cdb-2tp","type":"blocks","created_at":"2026-01-07T12:19:19.610572-07:00","created_by":"mayor"}]}
{"id":"cdb-1rl","title":"Merge: nux-mk4f1dyr","description":"branch: polecat/nux-mk4f1dyr\ntarget: main\nsource_issue: nux-mk4f1dyr\nrig: claudedb\nagent_bead: claude-db-claudedb-polecat-nux\nretry_count: 0\nlast_conflict_sha: null\nconflict_task_id: null","status":"open","priority":2,"issue_type":"merge-request","created_at":"2026-01-07T12:45:35.562316-07:00","created_by":"claudedb/polecats/nux","updated_at":"2026-01-07T14:05:19.909316-07:00"}
{"id":"cdb-25p","title":"dataPageIDs not persisted, Scan() returns empty after restart","description":"## Problem\n\nThe `dataPageIDs` field in `Table` is not persisted anywhere. After database restart, `Scan()` returns no rows even though data exists on disk.\n\n## Root Cause\n\n1. **TableInfo struct (catalog.go:40-46)** - Does not include `dataPageIDs`:\n   ```go\n   type TableInfo struct {\n       Name       string\n       RootPage   uint32\n       NextRowID  uint64\n       Columns    []ColumnInfo\n       PrimaryKey int\n       // dataPageIDs is MISSING!\n   }\n   ```\n\n2. **LoadTable (table.go)** - Creates empty slice:\n   ```go\n   func LoadTable(...) *Table {\n       return \u0026Table{\n           dataPageIDs: []uint32{},  // Always empty!\n       }\n   }\n   ```\n\n3. **Scan() (table.go:300-319)** - Iterates over `dataPageIDs`, which is empty after restart:\n   ```go\n   for _, pageID := range t.dataPageIDs {  // No iterations!\n       // ...\n   }\n   ```\n\n## Impact\n\nAfter database restart, SELECT queries return no rows. Data is still on disk but unreachable.\n\n## Suggested Fix\n\n**Option A: Persist dataPageIDs in catalog**\n- Add `DataPageIDs []uint32` to `TableInfo`\n- Serialize/deserialize in catalog read/write methods\n\n**Option B: Rebuild from B-tree on load**\n- Scan B-tree values (which contain pageID in upper 32 bits)\n- Collect unique page IDs to rebuild `dataPageIDs`\n```go\nfunc (t *Table) rebuildDataPageIDs() error {\n    _, values, _ := t.btree.Scan()\n    seen := make(map[uint32]bool)\n    for _, loc := range values {\n        pageID := uint32(loc \u003e\u003e 32)\n        if !seen[pageID] {\n            t.dataPageIDs = append(t.dataPageIDs, pageID)\n            seen[pageID] = true\n        }\n    }\n    return nil\n}\n```\n\n## Files Affected\n\n- internal/table/table.go\n- internal/catalog/catalog.go","status":"open","priority":1,"issue_type":"bug","created_at":"2026-01-04T06:10:50.588592-07:00","created_by":"cabewaldrop","updated_at":"2026-01-07T14:05:19.908007-07:00"}
{"id":"cdb-2tp","title":"Create query planner package","description":"Build a query planner that analyzes queries and chooses optimal execution strategy.\n\n## Current State\n- executor.go comment mentions planning but no planner exists\n- All queries go directly to execution with full scan\n\n## Implementation\n1. Create internal/sql/planner/ package\n2. Define QueryPlan interface with Execute() method\n3. Implement plan types:\n   - FullTableScan\n   - IndexScan (PK lookup)\n   - FilteredScan\n4. Planner analyzes WHERE clause + available indexes\n5. Executor calls planner.BuildPlan() then plan.Execute()\n\n## Example\n```go\ntype QueryPlan interface {\n    Execute(table *Table) ([]Row, error)\n}\n\ntype IndexScan struct {\n    index *BTree\n    key   []byte\n}\n\n// Usage in executor:\nplan := planner.BuildPlan(stmt, table)\nrows := plan.Execute(table)\n```\n\n## Files\n- NEW: internal/sql/planner/planner.go\n- NEW: internal/sql/planner/plans.go\n- MODIFY: internal/sql/executor/executor.go\n\n## Acceptance Criteria\n- Query planner package exists\n- Executor delegates access path choice to planner\n- Foundation for cost-based optimization","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-07T12:19:05.567201-07:00","created_by":"mayor","updated_at":"2026-01-07T14:05:19.910875-07:00","closed_at":"2026-01-07T12:21:32.770777-07:00","close_reason":"Superseded by existing claude-db-goh (Add query planner to analyze WHERE clauses) which has detailed design, edge cases, and test scenarios","dependencies":[{"issue_id":"cdb-2tp","depends_on_id":"cdb-epo","type":"blocks","created_at":"2026-01-07T12:19:19.419696-07:00","created_by":"mayor"},{"issue_id":"cdb-2tp","depends_on_id":"cdb-lyg","type":"blocks","created_at":"2026-01-07T12:19:19.454685-07:00","created_by":"mayor"}]}
{"id":"cdb-427","title":"nextRowID not properly persisted and restored on table load","description":"## Problem\n\nWhen a table is loaded from disk after restart, `nextRowID` is always reset to 1, causing duplicate row IDs.\n\n## Root Cause\n\nTwo issues:\n\n1. **catalog.go:300** - When saving table metadata, `NextRowID` is hardcoded to 1 instead of reading from the table:\n   ```go\n   info := \u0026TableInfo{\n       NextRowID:  1,  // BUG: should be tbl.nextRowID\n   }\n   ```\n\n2. **table.go LoadTable()** - The function doesn't accept `nextRowID` as a parameter and hardcodes it to 1:\n   ```go\n   func LoadTable(..., rootPage uint32) *Table {\n       return \u0026Table{\n           nextRowID: 1,  // BUG: ignores persisted value\n       }\n   }\n   ```\n\n3. **catalog.go:358** - `LoadTable` call doesn't pass `info.NextRowID`.\n\n## Impact\n\nAfter database restart, inserting new rows will reuse IDs 1, 2, 3... causing primary key conflicts or data corruption.\n\n## Suggested Fix\n\n1. Add a getter `Table.GetNextRowID()` or make field accessible\n2. Update `AddTable()` to read actual nextRowID from table\n3. Add `nextRowID` parameter to `LoadTable()`\n4. Pass `info.NextRowID` when loading tables\n\n## Files Affected\n\n- internal/table/table.go\n- internal/catalog/catalog.go","status":"open","priority":1,"issue_type":"bug","created_at":"2026-01-04T06:10:22.108089-07:00","created_by":"cabewaldrop","updated_at":"2026-01-07T14:05:19.908294-07:00"}
{"id":"cdb-5jc","title":"Add Table.GetRowByLocation() for index lookups","description":"## Problem\nThe B-tree `Search()` returns a row location as `(pageID \u003c\u003c 32 | offset)`, but Table has no method to retrieve a row by this location. This is a fundamental building block needed before any index-based query optimization can work.\n\n## Solution\nAdd a method to retrieve a single row by its physical location:\n\n```go\n// GetRowByLocation retrieves a row using the location from B-tree lookup.\n// Location format: (pageID \u003c\u003c 32) | offset\nfunc (t *Table) GetRowByLocation(location uint64) (Row, error) {\n    pageID := uint32(location \u003e\u003e 32)\n    offset := uint16(location \u0026 0xFFFF)\n    \n    page, err := t.pager.GetPage(pageID)\n    if err != nil {\n        return Row{}, fmt.Errorf(\"page %d not found: %w\", pageID, err)\n    }\n    \n    data := page.GetData()\n    if int(offset) \u003e= len(data)-1 {\n        return Row{}, fmt.Errorf(\"offset %d out of bounds for page %d\", offset, pageID)\n    }\n    \n    length := binary.LittleEndian.Uint16(data[offset:])\n    if length == 0 {\n        return Row{}, fmt.Errorf(\"invalid row at offset %d: zero length\", offset)\n    }\n    \n    endOffset := int(offset) + 2 + int(length)\n    if endOffset \u003e len(data) {\n        return Row{}, fmt.Errorf(\"row data exceeds page boundary\")\n    }\n    \n    rowData := data[offset+2 : endOffset]\n    return t.deserializeRow(rowData)\n}\n```\n\n## Files to Modify\n- internal/table/table.go\n\n## Edge Cases\n\n| Case | Input | Expected Behavior |\n|------|-------|-------------------|\n| Zero location | `location = 0` | Error: \"page 0 not found\" or valid if page 0 exists |\n| Invalid page ID | `pageID = 999999` | Error: \"page 999999 not found: ...\" |\n| Offset past data | `offset \u003e len(page.data)` | Error: \"offset N out of bounds for page M\" |\n| Zero-length row | `length = 0` at offset | Error: \"invalid row at offset N: zero length\" |\n| Row exceeds page | `offset + length \u003e pageSize` | Error: \"row data exceeds page boundary\" |\n| Valid location | Normal B-tree lookup result | Returns correct Row with matching ID |\n\n## Test Scenarios\n\n```go\nfunc TestGetRowByLocation(t *testing.T) {\n    // Setup: Create table, insert row, get location from B-tree\n    tbl := createTestTable(t)\n    rowID, _ := tbl.Insert([]Value{{Type: TypeInteger, Integer: 42}, {Type: TypeText, Text: \"hello\"}})\n    \n    // Get location via B-tree (this is how it will be used)\n    keyBytes := intToBytes(42)\n    location, found, _ := tbl.btree.Search(keyBytes)\n    require.True(t, found)\n    \n    // Test 1: Valid location returns correct row\n    row, err := tbl.GetRowByLocation(location)\n    require.NoError(t, err)\n    assert.Equal(t, rowID, row.ID)\n    assert.Equal(t, int64(42), row.Values[0].Integer)\n    assert.Equal(t, \"hello\", row.Values[1].Text)\n    \n    // Test 2: Invalid page ID\n    badLocation := uint64(999999) \u003c\u003c 32\n    _, err = tbl.GetRowByLocation(badLocation)\n    assert.ErrorContains(t, err, \"page 999999 not found\")\n    \n    // Test 3: Offset past page boundary\n    badOffset := uint64(pageID) \u003c\u003c 32 | uint64(5000)\n    _, err = tbl.GetRowByLocation(badOffset)\n    assert.ErrorContains(t, err, \"out of bounds\")\n}\n```\n\n## Agent Verification Checklist\n\nRun these commands and verify the expected outcomes:\n\n```bash\n# 1. Code compiles\ngo build ./...\n# Expected: exit code 0, no errors\n\n# 2. Unit tests pass\ngo test ./internal/table/... -v -run TestGetRowByLocation\n# Expected: PASS\n\n# 3. Method signature exists\ngrep -n \"func (t \\*Table) GetRowByLocation\" internal/table/table.go\n# Expected: Returns line number with correct signature\n\n# 4. Error handling exists (check for wrapped errors)\ngrep -c \"fmt.Errorf\" internal/table/table.go\n# Expected: At least 3 new error cases added\n```\n\n## Definition of Done\n- [ ] `GetRowByLocation(location uint64) (Row, error)` method exists on Table\n- [ ] All 5 edge cases from table above are handled with specific error messages\n- [ ] `TestGetRowByLocation` test file exists and passes\n- [ ] Integration test: Insert row → B-tree Search → GetRowByLocation → verify row matches\n- [ ] `go test ./internal/table/... -v` shows all tests passing","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-05T05:48:49.071027-07:00","created_by":"cabewaldrop","updated_at":"2026-01-07T14:05:19.907075-07:00","closed_at":"2026-01-07T13:17:11.510595-07:00","close_reason":"Implemented by furiosa in commit f6183b8: feat(table): add GetRowByLocation() for index lookups"}
{"id":"cdb-70r","title":"ORDER BY + LIMIT optimization","description":"Optimize ORDER BY when combined with LIMIT to avoid full sort.\n\n## Current Behavior\n- executor.go lines 357-374 sort ALL matching rows\n- Then applies LIMIT after sorting\n- Wasteful for 'SELECT ... ORDER BY x LIMIT 10' on 1M rows\n\n## Implementation Options\n1. **Index-ordered scan**: If ORDER BY matches an index, use index scan (naturally sorted)\n2. **Top-K heap**: For LIMIT K, maintain heap of K items instead of full sort\n3. **Push LIMIT into sort**: Stop sorting after K items found\n\n## Files\n- internal/sql/executor/executor.go (lines 357-374)\n- internal/sql/planner/ (detect ORDER BY + LIMIT pattern)\n\n## Acceptance Criteria\n- ORDER BY + LIMIT 10 on 1M rows doesn't sort all 1M\n- If index matches ORDER BY, use index scan order\n- Benchmark shows improvement for small LIMIT values","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-07T12:19:06.184708-07:00","created_by":"mayor","updated_at":"2026-01-07T14:05:19.913619-07:00","dependencies":[{"issue_id":"cdb-70r","depends_on_id":"cdb-2tp","type":"blocks","created_at":"2026-01-07T12:19:19.681676-07:00","created_by":"mayor"}]}
{"id":"cdb-7kz","title":"serializeValue missing default case silently corrupts data","description":"## Problem\n\n`serializeValue()` has no default case in its switch statement. Unknown types silently write no value data, causing deserialization to read misaligned garbage.\n\n## Root Cause\n\n**table.go:356-381**:\n```go\nfunc (t *Table) serializeValue(buf *bytes.Buffer, val Value) {\n    buf.WriteByte(byte(val.Type))  // Type written\n    buf.WriteByte(0)               // Null flag written\n    \n    switch val.Type {\n    case parser.TypeInteger:\n        binary.Write(buf, binary.LittleEndian, val.Integer)\n    case parser.TypeReal:\n        // ...\n    case parser.TypeText:\n        // ...\n    case parser.TypeBoolean:\n        // ...\n    // NO DEFAULT CASE!\n    }\n}\n```\n\nIf `val.Type` is `TypeUnknown` or any future type not handled, the type byte and null flag are written but no value bytes follow.\n\n## Impact\n\n- Deserialization reads wrong bytes (misaligned)\n- Silent data corruption\n- Difficult to debug — corruption happens at write time, error appears at read time\n\n## Suggested Fix\n\nChange signature to return error and add default case:\n\n```go\nfunc (t *Table) serializeValue(buf *bytes.Buffer, val Value) error {\n    buf.WriteByte(byte(val.Type))\n    if val.IsNull {\n        buf.WriteByte(1)\n        return nil\n    }\n    buf.WriteByte(0)\n\n    switch val.Type {\n    case parser.TypeInteger:\n        if err := binary.Write(buf, binary.LittleEndian, val.Integer); err != nil {\n            return err\n        }\n    // ... other cases with error handling ...\n    default:\n        return fmt.Errorf(\"unsupported type for serialization: %v\", val.Type)\n    }\n    return nil\n}\n```\n\nThis also addresses the ignored `binary.Write` errors in the same function.\n\n## Files Affected\n\n- internal/table/table.go (serializeValue and its callers)","status":"open","priority":2,"issue_type":"bug","created_at":"2026-01-04T06:26:39.398424-07:00","created_by":"cabewaldrop","updated_at":"2026-01-07T14:05:19.912784-07:00"}
{"id":"cdb-8p0","title":"Use B-tree for primary key equality lookups","description":"## Problem\nThe executor always does full table scans even for primary key lookups.\n\n## Solution\nUse query planner output to choose between index lookup and full scan.\n\n## Implementation\n\n```go\nfunc (e *Executor) executeSelect(stmt *parser.SelectStatement) (*Result, error) {\n    tbl, exists := e.tables[stmt.From]\n    if !exists {\n        return nil, fmt.Errorf(\"table %s does not exist\", stmt.From)\n    }\n    \n    // Get plan from planner\n    plan, err := e.planner.Plan(stmt)\n    if err != nil {\n        return nil, fmt.Errorf(\"planning failed: %w\", err)\n    }\n    \n    var rows []table.Row\n    \n    switch plan.Type {\n    case planner.PlanIndexScan:\n        // Single-key lookup\n        keyBytes := predicateToKey(plan.IndexPredicates[0])\n        location, found, err := tbl.GetBTree().Search(keyBytes)\n        if err != nil {\n            return nil, err\n        }\n        if found {\n            row, err := tbl.GetRowByLocation(location)\n            if err != nil {\n                return nil, err\n            }\n            // Apply residual predicates\n            if matchesPredicates(row, plan.ResidualPredicates, tbl.Schema) {\n                rows = append(rows, row)\n            }\n        }\n        \n    case planner.PlanIndexRangeScan:\n        // Range scan via iterator\n        start, end := predicatesToBounds(plan.IndexPredicates)\n        iter := tbl.GetBTree().RangeScan(start, end, storage.ScanOptions{})\n        defer iter.Close()\n        \n        for iter.Next() {\n            row, err := tbl.GetRowByLocation(iter.Value())\n            if err != nil {\n                continue // Skip corrupted rows\n            }\n            if matchesPredicates(row, plan.ResidualPredicates, tbl.Schema) {\n                rows = append(rows, row)\n                if plan.Limit != nil \u0026\u0026 len(rows) \u003e= *plan.Limit {\n                    break\n                }\n            }\n        }\n        if iter.Err() != nil {\n            return nil, iter.Err()\n        }\n        \n    case planner.PlanTableScan:\n        // Full scan (existing behavior)\n        allRows, err := tbl.Scan()\n        if err != nil {\n            return nil, err\n        }\n        for _, row := range allRows {\n            if matchesPredicates(row, plan.ResidualPredicates, tbl.Schema) {\n                rows = append(rows, row)\n            }\n        }\n    }\n    \n    // ... rest of SELECT (ORDER BY, LIMIT, projection)\n}\n```\n\n## Files to Modify\n- internal/sql/executor/executor.go\n\n## Edge Cases\n\n| Case | Query | Expected Behavior |\n|------|-------|-------------------|\n| PK equals literal | `WHERE id = 5` | IndexScan, returns 0 or 1 row |\n| PK not found | `WHERE id = 999` (no such row) | IndexScan, returns 0 rows (not error) |\n| PK + other condition | `WHERE id = 5 AND name = 'x'` | IndexScan, then filter by name |\n| PK range | `WHERE id \u003e 10` | IndexRangeScan |\n| Non-PK column | `WHERE name = 'x'` | TableScan with filter |\n| No WHERE | `SELECT * FROM t` | TableScan, no filter |\n| PK with LIMIT | `WHERE id = 5 LIMIT 1` | IndexScan (LIMIT is no-op for single key) |\n| Type mismatch | `WHERE id = 'five'` | Error or TableScan (depends on parser) |\n| Empty table | `WHERE id = 1` on empty table | IndexScan, returns 0 rows |\n\n## Test Scenarios\n\n```go\nfunc TestExecutorUsesPKIndex(t *testing.T) {\n    exec := setupExecutorWithTable(t, \"users\", \n        \"id INTEGER PRIMARY KEY, name TEXT\")\n    \n    // Insert test data\n    exec.Execute(parseSQL(\"INSERT INTO users VALUES (1, 'Alice')\"))\n    exec.Execute(parseSQL(\"INSERT INTO users VALUES (2, 'Bob')\"))\n    exec.Execute(parseSQL(\"INSERT INTO users VALUES (3, 'Charlie')\"))\n    \n    // Test: PK lookup should use index\n    result, err := exec.Execute(parseSQL(\"SELECT * FROM users WHERE id = 2\"))\n    require.NoError(t, err)\n    assert.Equal(t, 1, len(result.Rows))\n    assert.Equal(t, \"Bob\", result.Rows[0][1].Text)\n}\n\nfunc TestExecutorPKNotFound(t *testing.T) {\n    exec := setupExecutorWithTable(t, \"users\", \n        \"id INTEGER PRIMARY KEY, name TEXT\")\n    exec.Execute(parseSQL(\"INSERT INTO users VALUES (1, 'Alice')\"))\n    \n    result, err := exec.Execute(parseSQL(\"SELECT * FROM users WHERE id = 999\"))\n    require.NoError(t, err)\n    assert.Equal(t, 0, len(result.Rows)) // Not error, just empty\n}\n\nfunc TestExecutorMixedPredicates(t *testing.T) {\n    exec := setupExecutorWithTable(t, \"users\", \n        \"id INTEGER PRIMARY KEY, name TEXT, age INTEGER\")\n    exec.Execute(parseSQL(\"INSERT INTO users VALUES (1, 'Alice', 30)\"))\n    exec.Execute(parseSQL(\"INSERT INTO users VALUES (2, 'Bob', 25)\"))\n    \n    // PK match but residual fails\n    result, err := exec.Execute(parseSQL(\n        \"SELECT * FROM users WHERE id = 1 AND age = 99\"))\n    require.NoError(t, err)\n    assert.Equal(t, 0, len(result.Rows))\n    \n    // PK match and residual passes\n    result, err = exec.Execute(parseSQL(\n        \"SELECT * FROM users WHERE id = 1 AND age = 30\"))\n    require.NoError(t, err)\n    assert.Equal(t, 1, len(result.Rows))\n}\n```\n\n## Performance Verification\n\n```go\nfunc BenchmarkPKLookup(b *testing.B) {\n    exec := setupExecutorWithTable(b, \"users\", \"id INTEGER PRIMARY KEY, name TEXT\")\n    \n    // Insert 10,000 rows\n    for i := 0; i \u003c 10000; i++ {\n        exec.Execute(parseSQL(fmt.Sprintf(\n            \"INSERT INTO users VALUES (%d, 'User%d')\", i, i)))\n    }\n    \n    stmt := parseSQL(\"SELECT * FROM users WHERE id = 5000\")\n    \n    b.ResetTimer()\n    for i := 0; i \u003c b.N; i++ {\n        exec.Execute(stmt)\n    }\n    // Should be O(log n) ~ 14 comparisons, not O(n) = 10000 comparisons\n}\n```\n\n## Agent Verification Checklist\n\n```bash\n# 1. Executor uses planner\ngrep -n \"planner.Plan\" internal/sql/executor/executor.go\n# Expected: Shows planner being called in executeSelect\n\n# 2. Switch on plan type\ngrep -n \"switch plan.Type\" internal/sql/executor/executor.go\n# Expected: Shows switch statement with PlanIndexScan case\n\n# 3. Uses GetRowByLocation\ngrep -n \"GetRowByLocation\" internal/sql/executor/executor.go\n# Expected: Shows call after B-tree Search\n\n# 4. Tests pass\ngo test ./internal/sql/executor/... -v -run TestExecutor\n# Expected: All PASS\n\n# 5. Benchmark shows improvement\ngo test ./internal/sql/executor/... -bench=BenchmarkPKLookup -benchtime=1s\n# Expected: Fast (microseconds per op, not milliseconds)\n```\n\n## Definition of Done\n- [ ] `executeSelect` calls `planner.Plan()` to get execution plan\n- [ ] Switch statement handles `PlanIndexScan`, `PlanIndexRangeScan`, `PlanTableScan`\n- [ ] `PlanIndexScan` uses `btree.Search()` + `GetRowByLocation()`\n- [ ] Residual predicates applied after row fetch\n- [ ] `TestExecutorUsesPKIndex` passes: correct row returned\n- [ ] `TestExecutorPKNotFound` passes: empty result, no error\n- [ ] `TestExecutorMixedPredicates` passes: residual filtering works\n- [ ] Benchmark shows O(log n) behavior for PK lookups","status":"hooked","priority":1,"issue_type":"feature","assignee":"claudedb/polecats/dementus","created_at":"2026-01-05T05:38:48.750693-07:00","created_by":"cabewaldrop","updated_at":"2026-01-07T14:18:34.309423-07:00","dependencies":[{"issue_id":"cdb-8p0","depends_on_id":"cdb-5jc","type":"blocks","created_at":"2026-01-05T05:51:10.608296-07:00","created_by":"cabewaldrop"},{"issue_id":"cdb-8p0","depends_on_id":"cdb-goh","type":"blocks","created_at":"2026-01-05T05:51:15.852079-07:00","created_by":"cabewaldrop"}]}
{"id":"cdb-931","title":"Range scan support for inequality operators","description":"Enable index-backed range scans for inequality operators (\u003e, \u003c, \u003e=, \u003c=, BETWEEN).\n\n## Current State\n- B+ tree leaf nodes are linked (standard B+ tree design)\n- Only equality lookups possible\n- Inequalities always do full scan\n\n## Implementation\n1. Add BTree.RangeSearch(minKey, maxKey) method\n2. Traverse to start leaf, follow links to end\n3. Query planner detects range patterns in WHERE\n4. Generate RangeScan plan instead of FullTableScan\n\n## Patterns to Detect\n- column \u003e value\n- column \u003c value\n- column \u003e= value\n- column \u003c= value\n- column BETWEEN low AND high\n\n## Files\n- internal/storage/btree.go (RangeSearch method)\n- internal/sql/planner/ (range pattern detection)\n\n## Acceptance Criteria\n- WHERE age \u003e 30 uses range scan if index exists\n- BETWEEN uses single range scan\n- Page access proportional to result set, not table size","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-07T12:19:05.994904-07:00","created_by":"mayor","updated_at":"2026-01-07T14:05:19.91024-07:00","closed_at":"2026-01-07T12:21:33.136725-07:00","close_reason":"Superseded by existing claude-db-loe (Implement B-tree range scans) which has iterator design, edge cases, and depends on claude-db-a0f (sibling pointers)","dependencies":[{"issue_id":"cdb-931","depends_on_id":"cdb-2tp","type":"blocks","created_at":"2026-01-07T12:19:19.642926-07:00","created_by":"mayor"}]}
{"id":"cdb-a0f","title":"Add leaf sibling pointers to B-tree for efficient range scans","description":"## Problem\nThe current B+ tree has no sibling pointers between leaf nodes. Range scans must traverse up and down the tree for each leaf, making them O(k log n) instead of O(log n + k).\n\n## Solution\nAdd next/prev sibling pointers to leaf nodes:\n\n```go\ntype BTreeNode struct {\n    pageID      uint32\n    isLeaf      bool\n    numKeys     uint16\n    keys        [][]byte\n    values      []uint64\n    children    []uint32\n    nextLeaf    uint32  // 0 if rightmost leaf\n    prevLeaf    uint32  // 0 if leftmost leaf\n}\n```\n\n## Implementation Details\n\n### Serialization Format Change\n```\nExisting: [isLeaf:1][numKeys:2][numChildren:2][keys...][values/children...]\nNew:      [isLeaf:1][numKeys:2][numChildren:2][nextLeaf:4][prevLeaf:4][keys...][values/children...]\n```\n\n### Split Maintenance\nWhen splitting a leaf node:\n```go\nfunc (bt *BTree) splitChild(...) {\n    // Before: left \u003c-\u003e right_neighbor\n    // After:  left \u003c-\u003e new_sibling \u003c-\u003e right_neighbor\n    \n    sibling.nextLeaf = child.nextLeaf\n    sibling.prevLeaf = child.pageID\n    child.nextLeaf = sibling.pageID\n    \n    // Update right neighbor's prevLeaf\n    if sibling.nextLeaf != 0 {\n        rightNeighbor := loadNode(sibling.nextLeaf)\n        rightNeighbor.prevLeaf = sibling.pageID\n        saveNode(rightNeighbor)\n    }\n}\n```\n\n## Files to Modify\n- internal/storage/btree.go\n\n## Edge Cases\n\n| Case | Scenario | Expected Behavior |\n|------|----------|-------------------|\n| Single-node tree | Root is the only leaf | nextLeaf=0, prevLeaf=0 |\n| First split | Root splits into two leaves | left.nextLeaf=right, right.prevLeaf=left, both ends=0 |\n| Middle split | Split a middle leaf | Chain: ...prev \u003c-\u003e left \u003c-\u003e new \u003c-\u003e next... |\n| Leftmost leaf | First leaf in chain | prevLeaf=0, nextLeaf=second_leaf |\n| Rightmost leaf | Last leaf in chain | prevLeaf=second_to_last, nextLeaf=0 |\n| Load old format | B-tree without sibling fields | nextLeaf=0, prevLeaf=0 (graceful default) |\n\n## Migration Strategy\nFor existing B-trees created before this change:\n```go\nfunc deserializeNode(page *Page) (*BTreeNode, error) {\n    // ... read existing fields ...\n    \n    // Check if sibling fields exist (new format)\n    if buf.Len() \u003e= 8 {\n        binary.Read(buf, binary.LittleEndian, \u0026node.nextLeaf)\n        binary.Read(buf, binary.LittleEndian, \u0026node.prevLeaf)\n    } else {\n        // Old format: default to no siblings\n        node.nextLeaf = 0\n        node.prevLeaf = 0\n    }\n}\n```\n\n## Test Scenarios\n\n```go\nfunc TestSiblingPointersAfterInserts(t *testing.T) {\n    bt := createEmptyBTree(t)\n    \n    // Insert enough keys to cause multiple splits\n    for i := 0; i \u003c 250; i++ {\n        bt.Insert(intToBytes(i), uint64(i))\n    }\n    \n    // Verify sibling chain by traversing from leftmost to rightmost\n    leftmost := findLeftmostLeaf(bt)\n    assert.Equal(t, uint32(0), leftmost.prevLeaf, \"leftmost should have no prev\")\n    \n    visited := 0\n    current := leftmost\n    var lastKey []byte\n    for current != nil {\n        // Verify keys are in order across siblings\n        if lastKey != nil {\n            assert.True(t, bytes.Compare(lastKey, current.keys[0]) \u003c 0,\n                \"keys should be ordered across siblings\")\n        }\n        if len(current.keys) \u003e 0 {\n            lastKey = current.keys[len(current.keys)-1]\n        }\n        \n        visited++\n        if current.nextLeaf == 0 {\n            assert.Equal(t, uint32(0), current.nextLeaf, \"rightmost should have no next\")\n            break\n        }\n        current = loadNode(current.nextLeaf)\n    }\n    \n    // Count should match number of leaves\n    assert.True(t, visited \u003e= 3, \"should have multiple leaves after 250 inserts\")\n}\n\nfunc TestSiblingChainIntegrity(t *testing.T) {\n    bt := createBTreeWith100Keys(t)\n    \n    // Traverse forward and count\n    forwardCount := countLeavesForward(bt)\n    \n    // Traverse backward and count\n    backwardCount := countLeavesBackward(bt)\n    \n    assert.Equal(t, forwardCount, backwardCount, \"forward and backward traversal should match\")\n    \n    // Verify bidirectional links\n    current := findLeftmostLeaf(bt)\n    for current.nextLeaf != 0 {\n        next := loadNode(current.nextLeaf)\n        assert.Equal(t, current.pageID, next.prevLeaf, \n            \"next.prevLeaf should point back to current\")\n        current = next\n    }\n}\n```\n\n## Agent Verification Checklist\n\n```bash\n# 1. Struct has new fields\ngrep -A5 \"type BTreeNode struct\" internal/storage/btree.go | grep -E \"nextLeaf|prevLeaf\"\n# Expected: Both fields present\n\n# 2. Serialization includes siblings\ngrep -n \"nextLeaf\" internal/storage/btree.go | wc -l\n# Expected: At least 4 occurrences (struct, serialize, deserialize, split)\n\n# 3. Split maintains siblings\ngrep -B2 -A5 \"sibling.nextLeaf\" internal/storage/btree.go\n# Expected: Shows sibling chain maintenance code\n\n# 4. All existing tests still pass\ngo test ./internal/storage/... -v -run TestBTree\n# Expected: All PASS\n\n# 5. New sibling tests pass\ngo test ./internal/storage/... -v -run TestSibling\n# Expected: All PASS\n```\n\n## Definition of Done\n- [ ] `BTreeNode` struct has `nextLeaf uint32` and `prevLeaf uint32` fields\n- [ ] `serializeNode` writes sibling pointers (8 extra bytes)\n- [ ] `deserializeNode` reads sibling pointers (with backward compatibility)\n- [ ] `splitChild` maintains sibling chain (updates 3 nodes: left, new, right_neighbor)\n- [ ] `TestSiblingPointersAfterInserts` passes: forward traversal visits all leaves in order\n- [ ] `TestSiblingChainIntegrity` passes: prev/next pointers are bidirectional\n- [ ] All existing `TestBTree*` tests still pass (no regression)","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-05T05:48:54.324651-07:00","created_by":"cabewaldrop","updated_at":"2026-01-07T14:05:19.911295-07:00","closed_at":"2026-01-07T13:17:11.854707-07:00","close_reason":"Implemented by slit in commit 0b4d56c: Add leaf sibling pointers to B-tree for efficient range scans"}
{"id":"cdb-au8","title":"Add basic table/index statistics for query planning","description":"## Problem\nWithout statistics, the query planner cannot make informed decisions about which access path is best.\n\n## Solution\nTrack row counts and provide basic cardinality estimates.\n\n```go\ntype TableStats struct {\n    RowCount     int64     // Maintained on INSERT/DELETE\n    PageCount    int       // Number of data pages\n    LastAnalyzed time.Time // When ANALYZE was run\n}\n\ntype IndexStats struct {\n    DistinctKeys int64  // Approximate distinct key count\n    LeafPages    int    // Number of leaf pages in B-tree\n    TreeHeight   int    // B-tree depth\n}\n```\n\n## Implementation\n\n### Maintain RowCount\n```go\nfunc (t *Table) Insert(values []Value) (uint64, error) {\n    // ... existing insert logic ...\n    t.stats.RowCount++\n    return rowID, nil\n}\n\nfunc (t *Table) Delete(filter func(Row) bool) (int, error) {\n    // ... delete logic ...\n    t.stats.RowCount -= deletedCount\n    return deletedCount, nil\n}\n```\n\n### ANALYZE Command\n```sql\nANALYZE users;  -- Refresh statistics for 'users' table\nANALYZE;        -- Refresh all tables\n```\n\n```go\nfunc (e *Executor) executeAnalyze(tableName string) error {\n    tbl := e.tables[tableName]\n    \n    // Count rows (could sample for large tables)\n    rows, _ := tbl.Scan()\n    tbl.stats.RowCount = int64(len(rows))\n    tbl.stats.PageCount = len(tbl.dataPageIDs)\n    tbl.stats.LastAnalyzed = time.Now()\n    \n    // For each index, estimate distinct keys\n    for _, idx := range tbl.indexes {\n        keys, _, _ := idx.btree.Scan()\n        distinct := countDistinct(keys)\n        idx.stats.DistinctKeys = distinct\n        idx.stats.TreeHeight = measureTreeHeight(idx.btree)\n    }\n    \n    return e.catalog.SaveStats(tableName, tbl.stats)\n}\n```\n\n### Selectivity Estimation\n```go\nfunc (p *Planner) estimateSelectivity(pred Predicate, stats *TableStats) float64 {\n    switch pred.Operator {\n    case OpEq:\n        // Equality: assume uniform distribution\n        // selectivity = 1 / distinctValues\n        if stats.DistinctKeys \u003e 0 {\n            return 1.0 / float64(stats.DistinctKeys)\n        }\n        return 0.1 // Default 10%\n        \n    case OpLt, OpLe, OpGt, OpGe:\n        // Range: assume 30% selectivity (conservative)\n        return 0.3\n        \n    case OpNe:\n        // Not equal: 1 - equality selectivity\n        return 1.0 - p.estimateSelectivity(Predicate{Operator: OpEq}, stats)\n        \n    default:\n        return 0.5\n    }\n}\n\nfunc (p *Planner) estimateRows(plan *QueryPlan, stats *TableStats) float64 {\n    rows := float64(stats.RowCount)\n    for _, pred := range plan.IndexPredicates {\n        rows *= p.estimateSelectivity(pred, stats)\n    }\n    for _, pred := range plan.ResidualPredicates {\n        rows *= p.estimateSelectivity(pred, stats)\n    }\n    return math.Max(1, rows) // At least 1 row estimated\n}\n```\n\n## Files to Modify\n- internal/table/table.go (TableStats, maintain counts)\n- internal/catalog/catalog.go (persist stats)\n- internal/sql/parser/parser.go (ANALYZE statement)\n- internal/sql/executor/executor.go (executeAnalyze)\n- internal/sql/planner/planner.go (use stats)\n\n## Edge Cases\n\n| Case | Scenario | Expected Behavior |\n|------|----------|-------------------|\n| Empty table | RowCount=0 | Estimates return 0 or 1 rows |\n| No ANALYZE run | Stats not initialized | Use defaults (10% selectivity) |\n| After mass delete | RowCount might be stale | ANALYZE refreshes count |\n| Very skewed data | 99% of rows have same value | Estimates may be off (acceptable) |\n| NULL values | Column with many NULLs | Not specially handled in v1 |\n| Concurrent DML | INSERT during ANALYZE | Approximate count acceptable |\n\n## Test Scenarios\n\n```go\nfunc TestRowCountMaintained(t *testing.T) {\n    tbl := createTestTable(t)\n    assert.Equal(t, int64(0), tbl.Stats().RowCount)\n    \n    tbl.Insert(testRow1)\n    assert.Equal(t, int64(1), tbl.Stats().RowCount)\n    \n    tbl.Insert(testRow2)\n    tbl.Insert(testRow3)\n    assert.Equal(t, int64(3), tbl.Stats().RowCount)\n}\n\nfunc TestAnalyzeCommand(t *testing.T) {\n    exec := setupExecutorWithData(t, \"users\", 100) // 100 rows\n    \n    // Stats should be available after ANALYZE\n    _, err := exec.Execute(parseSQL(\"ANALYZE users\"))\n    require.NoError(t, err)\n    \n    tbl, _ := exec.GetTable(\"users\")\n    assert.Equal(t, int64(100), tbl.Stats().RowCount)\n    assert.True(t, tbl.Stats().LastAnalyzed.After(time.Now().Add(-time.Minute)))\n}\n\nfunc TestSelectivityEstimation(t *testing.T) {\n    planner := setupPlanner(t)\n    stats := \u0026TableStats{RowCount: 1000}\n    indexStats := \u0026IndexStats{DistinctKeys: 100}\n    \n    // Equality: 1/100 = 1%\n    sel := planner.estimateSelectivity(\n        Predicate{Operator: OpEq}, stats, indexStats)\n    assert.InDelta(t, 0.01, sel, 0.001)\n    \n    // Range: 30% default\n    sel = planner.estimateSelectivity(\n        Predicate{Operator: OpGt}, stats, indexStats)\n    assert.InDelta(t, 0.3, sel, 0.01)\n}\n\nfunc TestPlannerUsesStats(t *testing.T) {\n    // Table with 10000 rows, index with 100 distinct values\n    planner := setupPlannerWithStats(t, 10000, 100)\n    \n    plan, _ := planner.Plan(parseSelect(\"SELECT * FROM t WHERE id = 5\"))\n    \n    // Should estimate ~100 rows (10000 / 100 distinct)\n    assert.InDelta(t, 100.0, plan.EstimatedRows, 20.0)\n}\n```\n\n## Agent Verification Checklist\n\n```bash\n# 1. TableStats struct exists\ngrep -n \"type TableStats struct\" internal/table/table.go\n# Expected: Shows struct with RowCount field\n\n# 2. RowCount updated on Insert\ngrep -B2 -A2 \"stats.RowCount++\" internal/table/table.go\n# Expected: Shows increment in Insert method\n\n# 3. ANALYZE parses\ngrep -n \"ANALYZE\" internal/sql/parser/parser.go\n# Expected: Shows ANALYZE token handling\n\n# 4. Stats persisted to catalog\ngrep -n \"SaveStats\\|LoadStats\" internal/catalog/catalog.go\n# Expected: Shows persistence methods\n\n# 5. Tests pass\ngo test ./... -v -run \"TestRowCount\\|TestAnalyze\\|TestSelectivity\"\n# Expected: All PASS\n```\n\n## Definition of Done\n- [ ] `TableStats` struct with `RowCount`, `PageCount`, `LastAnalyzed`\n- [ ] `RowCount` incremented on `Insert()`, decremented on `Delete()`\n- [ ] `ANALYZE tablename` SQL command parses and executes\n- [ ] Stats persisted in catalog (survive restart)\n- [ ] `Planner.estimateSelectivity()` uses stats for cost estimation\n- [ ] `QueryPlan.EstimatedRows` populated based on stats\n- [ ] `TestRowCountMaintained` passes\n- [ ] `TestAnalyzeCommand` passes\n- [ ] `TestPlannerUsesStats` passes","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-05T05:48:59.577991-07:00","created_by":"cabewaldrop","updated_at":"2026-01-07T14:05:19.91492-07:00"}
{"id":"cdb-c0z","title":"Document binary layouts for all storage structures","description":"Create comprehensive documentation for all binary data layouts in claude-db with ASCII diagrams.\n\n## Structures to Document\n\n1. **Page Layout (4096 bytes)** - internal/storage/page.go\n   - 16-byte header: PageID (u32), PageType (u8), NumSlots (u16), FreeSpaceOffset (u16), Reserved (7 bytes)\n   - 4080-byte data area\n\n2. **B-Tree Node Layout** - internal/storage/btree.go\n   - Header: IsLeaf (u8), NumKeys (u16), NumChildren (u16)\n   - Keys: length-prefixed variable data\n   - Values (leaf): uint64 location encodings\n   - Children (internal): uint32 PageIDs\n\n3. **Table Row Layout** - internal/table/table.go\n   - Length prefix (u16)\n   - Row header: RowID (u64), NumValues (u16)\n   - Values: type-tagged (u8 type, u8 null flag, variable data)\n\n4. **Database Catalog (Page 0)** - internal/catalog/catalog.go\n   - Magic (0xCDB0), NumTables (u16)\n   - TableInfo: name, RootPage, NextRowID, PrimaryKey, columns\n   - ColumnInfo: name, type, flags (PrimaryKey/NotNull bits)\n\n5. **Row Location Encoding** - internal/table/table.go:290-291\n   - uint64: upper 32 bits = PageID, lower 32 bits = offset\n\n## Deliverables\n- docs/binary-format.md with ASCII tables for each layout\n- Byte offset annotations\n- Hex dump examples\n- Size constraints and limits\n- All little-endian, constants reference table","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-04T08:50:53.530093-07:00","created_by":"cabewaldrop","updated_at":"2026-01-07T14:05:19.912507-07:00","closed_at":"2026-01-04T09:41:45.926944-07:00","close_reason":"Closed"}
{"id":"cdb-ce9","title":"Merge: slit-mk4f1pp1","description":"branch: polecat/slit-mk4f1pp1\ntarget: main\nsource_issue: slit-mk4f1pp1\nrig: claudedb\nagent_bead: claude-db-claudedb-polecat-slit\nretry_count: 0\nlast_conflict_sha: null\nconflict_task_id: null","status":"open","priority":2,"issue_type":"merge-request","created_at":"2026-01-07T12:44:38.774721-07:00","created_by":"claudedb/polecats/slit","updated_at":"2026-01-07T14:05:19.909557-07:00"}
{"id":"cdb-claudedb-polecat-capable","title":"cdb-claudedb-polecat-capable","description":"cdb-claudedb-polecat-capable\n\nrole_type: polecat\nrig: claudedb\nagent_state: spawning\nhook_bead: cdb-loe\nrole_bead: hq-polecat-role\ncleanup_status: null\nactive_mr: null\nnotification_level: null","status":"open","priority":2,"issue_type":"agent","created_at":"2026-01-07T14:18:58.919684-07:00","created_by":"mayor","updated_at":"2026-01-07T14:19:01.78056-07:00"}
{"id":"cdb-claudedb-polecat-dementus","title":"cdb-claudedb-polecat-dementus","description":"cdb-claudedb-polecat-dementus\n\nrole_type: polecat\nrig: claudedb\nagent_state: spawning\nhook_bead: cdb-8p0\nrole_bead: hq-polecat-role\ncleanup_status: null\nactive_mr: null\nnotification_level: null","status":"open","priority":2,"issue_type":"agent","created_at":"2026-01-07T14:05:57.764374-07:00","created_by":"mayor","updated_at":"2026-01-07T14:05:57.922306-07:00"}
{"id":"cdb-cti","title":"Merge: nux-btree","description":"branch: polecat/nux-btree-range\ntarget: main\nsource_issue: nux-btree\nrig: claudedb\nagent_bead: claude-db-claudedb-polecat-nux\nretry_count: 0\nlast_conflict_sha: null\nconflict_task_id: null","status":"open","priority":2,"issue_type":"merge-request","created_at":"2026-01-07T13:38:31.279646-07:00","created_by":"claudedb/polecats/nux","updated_at":"2026-01-07T14:05:19.909064-07:00"}
{"id":"cdb-doj","title":"Query Engine Performance Improvements","description":"Currently all queries use full table scans. The B+ tree index infrastructure exists but is never consulted during SELECT execution. This epic tracks work to build a proper query planner and leverage existing index infrastructure.\n\n## Current State\n- executor.go:299 always calls tbl.Scan() regardless of WHERE clause\n- B+ tree indexes are populated on INSERT but never used for lookups\n- ScanWithFilter() method exists but is unused\n- No query planner - queries go straight to executor\n\n## Goals\n- Use existing PK index for equality lookups\n- Build query planner to choose access paths\n- Support secondary indexes\n- Range scan support\n- ORDER BY + LIMIT optimization\n- Bounded buffer pool with LRU eviction\n\n## Key Files\n- internal/sql/executor/executor.go\n- internal/storage/btree.go\n- internal/table/table.go\n- internal/storage/pager.go","status":"open","priority":1,"issue_type":"epic","created_at":"2026-01-07T12:18:19.205962-07:00","created_by":"mayor","updated_at":"2026-01-07T14:05:19.906618-07:00","dependencies":[{"issue_id":"cdb-doj","depends_on_id":"cdb-lyg","type":"blocks","created_at":"2026-01-07T12:19:53.805623-07:00","created_by":"mayor"},{"issue_id":"cdb-doj","depends_on_id":"cdb-70r","type":"blocks","created_at":"2026-01-07T12:19:53.926971-07:00","created_by":"mayor"},{"issue_id":"cdb-doj","depends_on_id":"cdb-xa7","type":"blocks","created_at":"2026-01-07T12:19:53.95799-07:00","created_by":"mayor"},{"issue_id":"cdb-doj","depends_on_id":"cdb-8p0","type":"blocks","created_at":"2026-01-07T12:21:47.533211-07:00","created_by":"mayor"},{"issue_id":"cdb-doj","depends_on_id":"cdb-xsr","type":"blocks","created_at":"2026-01-07T12:21:47.563981-07:00","created_by":"mayor"},{"issue_id":"cdb-doj","depends_on_id":"cdb-loe","type":"blocks","created_at":"2026-01-07T12:21:47.599377-07:00","created_by":"mayor"}]}
{"id":"cdb-epo","title":"Use PK index for equality lookups","description":"Modify executeSelect() to detect simple primary key equality in WHERE clause and use btree.Search() instead of tbl.Scan().\n\n## Current Behavior\n- Line 299: `rows, err := tbl.Scan()` always does full scan\n- Even `WHERE id = 5` scans entire table\n\n## Implementation\n1. Before calling Scan(), analyze WHERE clause\n2. Detect pattern: BinaryExpression with OpEquals on PK column\n3. If detected, call btree.Search(key) instead\n4. Reduces complexity from O(n) to O(log n)\n\n## Files\n- internal/sql/executor/executor.go (line ~280-299)\n\n## Acceptance Criteria\n- SELECT with PK equality uses index lookup\n- Benchmark shows page access reduction (e.g., 4 pages vs 250 for 1M rows)","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-07T12:19:05.162456-07:00","created_by":"mayor","updated_at":"2026-01-07T14:05:19.906253-07:00","closed_at":"2026-01-07T12:21:32.610379-07:00","close_reason":"Superseded by existing claude-db-8p0 (Use B-tree for primary key equality lookups) which has detailed edge cases, tests, and proper dependencies"}
{"id":"cdb-f7o","title":"Add EXPLAIN command to show query plans","description":"## Problem\nUsers have no visibility into how queries are executed.\n\n## Solution\nAdd EXPLAIN statement that shows the query plan without executing.\n\n## Syntax\n```sql\nEXPLAIN SELECT * FROM users WHERE id = 5;\nEXPLAIN SELECT * FROM users WHERE name = 'Alice' AND age \u003e 30;\n```\n\n## Output Format\n```\nPlan Type: IndexScan\nTable: users\nIndex: PRIMARY\nIndex Condition: id = 5\nEstimated Rows: 1\nEstimated Cost: 1.0\n\n---\n\nPlan Type: TableScan\nTable: users  \nFilter: name = 'Alice' AND age \u003e 30\nEstimated Rows: 100\nEstimated Cost: 1000.0\n```\n\n## Implementation\n\n```go\n// AST\ntype ExplainStatement struct {\n    Statement Statement\n}\n\n// Parser addition\nfunc (p *Parser) parseExplain() (*ExplainStatement, error) {\n    p.expect(TokenExplain)\n    \n    // Parse the inner statement\n    innerStmt, err := p.parseStatement()\n    if err != nil {\n        return nil, err\n    }\n    \n    return \u0026ExplainStatement{Statement: innerStmt}, nil\n}\n\n// Executor\nfunc (e *Executor) executeExplain(stmt *ExplainStatement) (*Result, error) {\n    plan, err := e.planner.Plan(stmt.Statement)\n    if err != nil {\n        return nil, err\n    }\n    \n    return \u0026Result{\n        Message: plan.Explain(),\n    }, nil\n}\n\n// QueryPlan.Explain()\nfunc (p *QueryPlan) Explain() string {\n    var sb strings.Builder\n    \n    sb.WriteString(fmt.Sprintf(\"Plan Type: %s\\n\", p.Type))\n    sb.WriteString(fmt.Sprintf(\"Table: %s\\n\", p.Table))\n    \n    if p.Index != nil {\n        sb.WriteString(fmt.Sprintf(\"Index: %s\\n\", p.Index.Name))\n    }\n    \n    if len(p.IndexPredicates) \u003e 0 {\n        sb.WriteString(\"Index Condition: \")\n        sb.WriteString(formatPredicates(p.IndexPredicates))\n        sb.WriteString(\"\\n\")\n    }\n    \n    if len(p.ResidualPredicates) \u003e 0 {\n        sb.WriteString(\"Filter: \")\n        sb.WriteString(formatPredicates(p.ResidualPredicates))\n        sb.WriteString(\"\\n\")\n    }\n    \n    sb.WriteString(fmt.Sprintf(\"Estimated Rows: %.0f\\n\", p.EstimatedRows))\n    sb.WriteString(fmt.Sprintf(\"Estimated Cost: %.1f\\n\", p.EstimatedCost))\n    \n    return sb.String()\n}\n```\n\n## Files to Modify\n- internal/sql/lexer/lexer.go (TokenExplain)\n- internal/sql/parser/ast.go (ExplainStatement)\n- internal/sql/parser/parser.go (parseExplain)\n- internal/sql/executor/executor.go (executeExplain)\n- internal/sql/planner/planner.go (Explain method)\n\n## Edge Cases\n\n| Case | Input | Expected Output |\n|------|-------|-----------------|\n| Explain SELECT | `EXPLAIN SELECT * FROM t` | Shows plan for SELECT |\n| Explain UPDATE | `EXPLAIN UPDATE t SET x=1` | Shows plan for UPDATE |\n| Explain DELETE | `EXPLAIN DELETE FROM t` | Shows plan for DELETE |\n| Table not found | `EXPLAIN SELECT * FROM nosuchtable` | Error: \"table nosuchtable does not exist\" |\n| Syntax error in inner | `EXPLAIN SELEC * FROM t` | Parse error on inner statement |\n| EXPLAIN EXPLAIN | `EXPLAIN EXPLAIN SELECT...` | Error or shows plan for EXPLAIN |\n| EXPLAIN INSERT | `EXPLAIN INSERT INTO t...` | Plan or \"not supported\" (INSERT doesn't scan) |\n\n## Test Scenarios\n\n```go\nfunc TestExplainSelect(t *testing.T) {\n    exec := setupExecutorWithTable(t, \"users\", \n        \"id INTEGER PRIMARY KEY, name TEXT\")\n    \n    result, err := exec.Execute(parseSQL(\"EXPLAIN SELECT * FROM users WHERE id = 5\"))\n    require.NoError(t, err)\n    \n    output := result.Message\n    assert.Contains(t, output, \"Plan Type: IndexScan\")\n    assert.Contains(t, output, \"Table: users\")\n    assert.Contains(t, output, \"Index: PRIMARY\")\n    assert.Contains(t, output, \"Index Condition: id = 5\")\n    assert.Contains(t, output, \"Estimated Rows:\")\n}\n\nfunc TestExplainTableScan(t *testing.T) {\n    exec := setupExecutorWithTable(t, \"users\", \n        \"id INTEGER PRIMARY KEY, name TEXT\")\n    \n    result, err := exec.Execute(parseSQL(\n        \"EXPLAIN SELECT * FROM users WHERE name = 'Alice'\"))\n    require.NoError(t, err)\n    \n    output := result.Message\n    assert.Contains(t, output, \"Plan Type: TableScan\")\n    assert.Contains(t, output, \"Filter: name = 'Alice'\")\n    assert.NotContains(t, output, \"Index:\") // No index used\n}\n\nfunc TestExplainNonExistentTable(t *testing.T) {\n    exec := setupExecutor(t)\n    \n    _, err := exec.Execute(parseSQL(\"EXPLAIN SELECT * FROM nosuchtable\"))\n    assert.ErrorContains(t, err, \"does not exist\")\n}\n\nfunc TestExplainDoesNotExecute(t *testing.T) {\n    exec := setupExecutorWithTable(t, \"users\", \"id INTEGER PRIMARY KEY\")\n    \n    // Insert a row\n    exec.Execute(parseSQL(\"INSERT INTO users VALUES (1)\"))\n    \n    // EXPLAIN DELETE should NOT actually delete\n    _, err := exec.Execute(parseSQL(\"EXPLAIN DELETE FROM users WHERE id = 1\"))\n    require.NoError(t, err)\n    \n    // Row should still exist\n    result, _ := exec.Execute(parseSQL(\"SELECT * FROM users\"))\n    assert.Equal(t, 1, len(result.Rows))\n}\n```\n\n## Agent Verification Checklist\n\n```bash\n# 1. EXPLAIN token defined\ngrep -n \"TokenExplain\\|EXPLAIN\" internal/sql/lexer/lexer.go\n# Expected: Shows token definition\n\n# 2. ExplainStatement in AST\ngrep -n \"type ExplainStatement struct\" internal/sql/parser/ast.go\n# Expected: Shows struct definition\n\n# 3. parseExplain function exists\ngrep -n \"func.*parseExplain\" internal/sql/parser/parser.go\n# Expected: Shows parser function\n\n# 4. Explain method on QueryPlan\ngrep -n \"func.*QueryPlan.*Explain\" internal/sql/planner/planner.go\n# Expected: Shows Explain method\n\n# 5. Tests pass\ngo test ./... -v -run \"TestExplain\"\n# Expected: All PASS\n```\n\n## Output Format Specification (exact format for verification)\n\n```\nPlan Type: \u003cIndexScan|IndexRangeScan|TableScan\u003e\nTable: \u003ctable_name\u003e\n[Index: \u003cindex_name\u003e]\n[Index Condition: \u003cpredicate\u003e[ AND \u003cpredicate\u003e...]]\n[Filter: \u003cpredicate\u003e[ AND \u003cpredicate\u003e...]]\nEstimated Rows: \u003cnumber\u003e\nEstimated Cost: \u003cnumber\u003e\n```\n\nLines in brackets are optional (only shown when applicable).\n\n## Definition of Done\n- [ ] `EXPLAIN` keyword lexes as `TokenExplain`\n- [ ] `ExplainStatement` struct wraps inner statement\n- [ ] `parseExplain()` correctly parses `EXPLAIN SELECT/UPDATE/DELETE`\n- [ ] `executeExplain()` calls planner but does NOT execute the inner query\n- [ ] `QueryPlan.Explain()` returns formatted string matching specification\n- [ ] Output contains: Plan Type, Table, (Index), (Index Condition), (Filter), Estimated Rows, Cost\n- [ ] `TestExplainSelect` passes: shows IndexScan for PK query\n- [ ] `TestExplainTableScan` passes: shows TableScan for non-indexed query\n- [ ] `TestExplainDoesNotExecute` passes: EXPLAIN DELETE doesn't delete rows","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-05T05:49:04.854487-07:00","created_by":"cabewaldrop","updated_at":"2026-01-07T14:05:19.91417-07:00","dependencies":[{"issue_id":"cdb-f7o","depends_on_id":"cdb-goh","type":"blocks","created_at":"2026-01-05T05:51:31.599261-07:00","created_by":"cabewaldrop"}]}
{"id":"cdb-fcu","title":"Merge: furiosa-mk4ewicq","description":"branch: polecat/furiosa-mk4ewicq\ntarget: main\nsource_issue: furiosa-mk4ewicq\nrig: claudedb\nagent_bead: claude-db-claudedb-polecat-furiosa\nretry_count: 0\nlast_conflict_sha: null\nconflict_task_id: null","status":"open","priority":2,"issue_type":"merge-request","created_at":"2026-01-07T12:42:52.002358-07:00","created_by":"claudedb/polecats/furiosa","updated_at":"2026-01-07T14:05:19.909977-07:00"}
{"id":"cdb-goh","title":"Add query planner to analyze WHERE clauses","description":"## Problem\nThere is no query planner - the executor directly executes queries without analyzing optimization opportunities.\n\n## Solution\nCreate a query planner that analyzes WHERE clauses and produces execution plans.\n\n## Design\n\n```go\ntype PlanType int\nconst (\n    PlanTableScan PlanType = iota\n    PlanIndexScan      // Exact key lookup via B-tree Search\n    PlanIndexRangeScan // Range scan via B-tree iterator\n)\n\ntype QueryPlan struct {\n    Type               PlanType\n    Table              string\n    Index              *IndexInfo          // nil for TableScan\n    IndexPredicates    []Predicate         // Pushed to index\n    ResidualPredicates []Predicate         // Applied post-fetch\n    Projection         []int               // Column indices (nil = all)\n    Limit              *int\n    EstimatedRows      float64\n    EstimatedCost      float64\n}\n\ntype Predicate struct {\n    Column   string\n    Operator CompareOp  // Eq, Lt, Gt, Le, Ge, Ne\n    Value    interface{}\n}\n\ntype Planner struct {\n    catalog *catalog.Catalog\n}\n\nfunc (p *Planner) Plan(stmt parser.Statement) (*QueryPlan, error)\nfunc (plan *QueryPlan) Explain() string\n```\n\n## Predicate Classification\n\n**Index-usable (SARGable)**:\n- `column = literal` → IndexScan\n- `column \u003e literal`, `column \u003c literal`, `column \u003e= literal`, `column \u003c= literal` → IndexRangeScan\n- `column BETWEEN a AND b` → IndexRangeScan\n\n**NOT index-usable (residual)**:\n- `column != literal` (requires full scan to find non-matches)\n- `column + 1 = 5` (expression on column)\n- `func(column) = value` (function on column)  \n- `column1 = column2` (no literal)\n- `column LIKE '%foo'` (leading wildcard)\n\n## Files to Create/Modify\n- internal/sql/planner/planner.go (new)\n- internal/sql/planner/predicate.go (new)\n- internal/sql/executor/executor.go\n\n## Edge Cases\n\n| Case | Query | Expected Plan |\n|------|-------|---------------|\n| No WHERE | `SELECT * FROM t` | TableScan, no predicates |\n| PK equality | `SELECT * FROM t WHERE id = 5` | IndexScan on PRIMARY |\n| PK range | `SELECT * FROM t WHERE id \u003e 10` | IndexRangeScan on PRIMARY |\n| Non-indexed column | `SELECT * FROM t WHERE name = 'x'` | TableScan, residual: [name='x'] |\n| AND with PK | `SELECT * FROM t WHERE id = 5 AND name = 'x'` | IndexScan on PRIMARY, residual: [name='x'] |\n| OR condition | `SELECT * FROM t WHERE id = 1 OR id = 2` | TableScan (OR not optimized in v1) |\n| NOT condition | `SELECT * FROM t WHERE NOT id = 5` | TableScan, residual: [NOT id=5] |\n| Expression on column | `SELECT * FROM t WHERE id + 1 = 5` | TableScan, residual: [id+1=5] |\n| NULL check | `SELECT * FROM t WHERE id IS NULL` | TableScan (B-tree doesn't index NULLs) |\n| Empty table | `SELECT * FROM empty_t WHERE id = 1` | IndexScan, EstimatedRows: 0 |\n| Non-existent table | `SELECT * FROM nosuchtable` | Error: \"table nosuchtable does not exist\" |\n\n## Test Scenarios\n\n```go\nfunc TestPlannerSelectPK(t *testing.T) {\n    // Setup: Table 'users' with PK on 'id'\n    planner := setupPlannerWithTable(t, \"users\", []Column{\n        {Name: \"id\", Type: TypeInteger, PrimaryKey: true},\n        {Name: \"name\", Type: TypeText},\n    })\n    \n    stmt := parseSelect(\"SELECT * FROM users WHERE id = 42\")\n    plan, err := planner.Plan(stmt)\n    \n    require.NoError(t, err)\n    assert.Equal(t, PlanIndexScan, plan.Type)\n    assert.Equal(t, \"users\", plan.Table)\n    assert.Equal(t, \"PRIMARY\", plan.Index.Name)\n    assert.Len(t, plan.IndexPredicates, 1)\n    assert.Equal(t, \"id\", plan.IndexPredicates[0].Column)\n    assert.Equal(t, OpEq, plan.IndexPredicates[0].Operator)\n    assert.Equal(t, int64(42), plan.IndexPredicates[0].Value)\n    assert.Empty(t, plan.ResidualPredicates)\n}\n\nfunc TestPlannerMixedPredicates(t *testing.T) {\n    stmt := parseSelect(\"SELECT * FROM users WHERE id = 42 AND name = 'Alice'\")\n    plan, err := planner.Plan(stmt)\n    \n    require.NoError(t, err)\n    assert.Equal(t, PlanIndexScan, plan.Type)\n    assert.Len(t, plan.IndexPredicates, 1)      // id = 42\n    assert.Len(t, plan.ResidualPredicates, 1)   // name = 'Alice'\n}\n\nfunc TestPlannerExplain(t *testing.T) {\n    stmt := parseSelect(\"SELECT * FROM users WHERE id = 42 AND name = 'Alice'\")\n    plan, _ := planner.Plan(stmt)\n    \n    explain := plan.Explain()\n    assert.Contains(t, explain, \"IndexScan\")\n    assert.Contains(t, explain, \"PRIMARY\")\n    assert.Contains(t, explain, \"Index Cond: id = 42\")\n    assert.Contains(t, explain, \"Filter: name = 'Alice'\")\n}\n```\n\n## Agent Verification Checklist\n\n```bash\n# 1. Package exists and compiles\ngo build ./internal/sql/planner/...\n# Expected: exit code 0\n\n# 2. Planner struct and Plan method exist\ngrep -n \"func (p \\*Planner) Plan\" internal/sql/planner/planner.go\n# Expected: Returns line number\n\n# 3. All tests pass\ngo test ./internal/sql/planner/... -v\n# Expected: PASS for all tests\n\n# 4. Executor uses planner\ngrep -n \"planner.Plan\" internal/sql/executor/executor.go\n# Expected: Returns line number(s) showing planner is called\n```\n\n## Definition of Done\n- [ ] `internal/sql/planner/` package exists with `planner.go` and `predicate.go`\n- [ ] `Planner.Plan()` returns correct `PlanType` for all 10 edge cases in table\n- [ ] `Explain()` output includes: plan type, index name, index conditions, filter conditions\n- [ ] Executor's `executeSelect` calls `planner.Plan()` before choosing access method\n- [ ] `go test ./internal/sql/planner/... -v` passes\n- [ ] At least 5 test cases covering: TableScan, IndexScan, mixed predicates, error cases","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-05T05:39:23.697454-07:00","created_by":"cabewaldrop","updated_at":"2026-01-07T14:05:19.912145-07:00","closed_at":"2026-01-07T13:17:11.677968-07:00","close_reason":"Implemented by nux in commit b70a1d9: Add query planner to analyze WHERE clauses"}
{"id":"cdb-ioq","title":"Session ended: gt-claudedb-refinery","status":"open","priority":2,"issue_type":"event","created_at":"2026-01-07T14:19:55.106581-07:00","created_by":"claudedb/refinery","updated_at":"2026-01-07T14:19:55.106581-07:00"}
{"id":"cdb-kns","title":"Merge: furiosa-mk4ewicq","description":"branch: polecat/furiosa-mk4ewicq\ntarget: main\nsource_issue: furiosa-mk4ewicq\nrig: claudedb\nagent_bead: claude-db-claudedb-polecat-furiosa\nretry_count: 0\nlast_conflict_sha: null\nconflict_task_id: null","status":"open","priority":2,"issue_type":"merge-request","created_at":"2026-01-07T13:38:52.414811-07:00","created_by":"claudedb/polecats/furiosa","updated_at":"2026-01-07T14:05:19.9088-07:00"}
{"id":"cdb-loe","title":"Implement B-tree range scans","description":"## Problem\nB-tree only supports exact key lookup. Range queries like `WHERE id \u003e 10 AND id \u003c 100` still do full table scans.\n\n## Solution\nAdd iterator-based range scan with early termination support.\n\n```go\ntype ScanOptions struct {\n    StartInclusive bool\n    EndInclusive   bool\n    Limit          int   // 0 = unlimited\n    Reverse        bool  // Descending order\n}\n\ntype BTreeIterator interface {\n    Next() bool           // Advance to next key\n    Key() []byte          // Current key (valid after Next returns true)\n    Value() uint64        // Current value\n    Err() error           // Any error during iteration\n    Close()               // Release resources\n}\n\n// RangeScan returns iterator over [start, end]\n// nil start = scan from beginning, nil end = scan to end\nfunc (bt *BTree) RangeScan(start, end []byte, opts ScanOptions) BTreeIterator\n```\n\n## Implementation\n\n```go\ntype btreeIterator struct {\n    bt           *BTree\n    currentNode  *BTreeNode\n    keyIndex     int\n    endKey       []byte\n    endInclusive bool\n    limit        int\n    count        int\n    exhausted    bool\n    err          error\n}\n\nfunc (it *btreeIterator) Next() bool {\n    if it.exhausted || it.err != nil {\n        return false\n    }\n    if it.limit \u003e 0 \u0026\u0026 it.count \u003e= it.limit {\n        it.exhausted = true\n        return false\n    }\n    \n    // Advance within current node\n    it.keyIndex++\n    \n    // Move to next leaf via sibling pointer\n    if it.keyIndex \u003e= int(it.currentNode.numKeys) {\n        if it.currentNode.nextLeaf == 0 {\n            it.exhausted = true\n            return false\n        }\n        page, err := it.bt.pager.GetPage(it.currentNode.nextLeaf)\n        if err != nil {\n            it.err = err\n            return false\n        }\n        it.currentNode, it.err = deserializeNode(page)\n        if it.err != nil {\n            return false\n        }\n        it.keyIndex = 0\n    }\n    \n    // Check end bound\n    if it.endKey != nil {\n        cmp := bytes.Compare(it.currentNode.keys[it.keyIndex], it.endKey)\n        if cmp \u003e 0 || (cmp == 0 \u0026\u0026 !it.endInclusive) {\n            it.exhausted = true\n            return false\n        }\n    }\n    \n    it.count++\n    return true\n}\n```\n\n## Files to Modify\n- internal/storage/btree.go\n- internal/storage/btree_iterator.go (new)\n\n## Edge Cases\n\n| Case | Start | End | Data | Expected |\n|------|-------|-----|------|----------|\n| Empty range | 50 | 40 | [1..100] | 0 results (start \u003e end) |\n| No matches | 200 | 300 | [1..100] | 0 results |\n| Single result | 50 | 50, inclusive | [1..100] | 1 result: key=50 |\n| Full scan | nil | nil | [1..100] | 100 results |\n| Open start | nil | 10 | [1..100] | 10 results: 1-10 |\n| Open end | 90 | nil | [1..100] | 11 results: 90-100 |\n| Exclusive bounds | 10 | 20, exclusive | [1..100] | 9 results: 11-19 |\n| With Limit | nil | nil, limit=5 | [1..100] | 5 results: 1-5 |\n| Iterator after Close | - | - | - | Next() returns false, Err() nil |\n| Empty tree | nil | nil | [] | 0 results |\n\n## Test Scenarios\n\n```go\nfunc TestRangeScanBasic(t *testing.T) {\n    bt := createBTreeWithKeys(t, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n    \n    iter := bt.RangeScan(intToBytes(3), intToBytes(7), ScanOptions{\n        StartInclusive: true,\n        EndInclusive:   true,\n    })\n    defer iter.Close()\n    \n    var results []int\n    for iter.Next() {\n        results = append(results, bytesToInt(iter.Key()))\n    }\n    require.NoError(t, iter.Err())\n    assert.Equal(t, []int{3, 4, 5, 6, 7}, results)\n}\n\nfunc TestRangeScanWithLimit(t *testing.T) {\n    bt := createBTreeWithKeys(t, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n    \n    iter := bt.RangeScan(nil, nil, ScanOptions{Limit: 3})\n    defer iter.Close()\n    \n    var results []int\n    for iter.Next() {\n        results = append(results, bytesToInt(iter.Key()))\n    }\n    assert.Equal(t, []int{1, 2, 3}, results)\n}\n\nfunc TestRangeScanEmptyRange(t *testing.T) {\n    bt := createBTreeWithKeys(t, 1, 2, 3, 4, 5)\n    \n    // Start \u003e End\n    iter := bt.RangeScan(intToBytes(10), intToBytes(5), ScanOptions{})\n    assert.False(t, iter.Next())\n    assert.NoError(t, iter.Err())\n}\n\nfunc TestRangeScanUsessSiblingPointers(t *testing.T) {\n    // Insert enough to have multiple leaves\n    bt := createBTreeWithKeys(t, rangeSlice(1, 300)...)\n    \n    iter := bt.RangeScan(intToBytes(50), intToBytes(250), ScanOptions{\n        StartInclusive: true,\n        EndInclusive:   true,\n    })\n    defer iter.Close()\n    \n    count := 0\n    for iter.Next() {\n        count++\n    }\n    assert.Equal(t, 201, count) // 50 to 250 inclusive\n    \n    // Verify we used sibling pointers (not re-traversing)\n    // This would be verified by checking we didn't call Search() multiple times\n    // or by counting page reads\n}\n```\n\n## Performance Requirement\nRange scan of K keys in tree of N total keys must be O(log N + K), not O(K log N).\n\nSimple verification:\n```go\nfunc BenchmarkRangeScan(b *testing.B) {\n    bt := createBTreeWithKeys(b, rangeSlice(1, 100000)...)\n    \n    b.ResetTimer()\n    for i := 0; i \u003c b.N; i++ {\n        iter := bt.RangeScan(intToBytes(1000), intToBytes(2000), ScanOptions{})\n        for iter.Next() {}\n        iter.Close()\n    }\n    // Should be ~same speed regardless of tree size if O(log N + K)\n}\n```\n\n## Agent Verification Checklist\n\n```bash\n# 1. Iterator interface defined\ngrep -n \"type BTreeIterator interface\" internal/storage/btree*.go\n# Expected: Shows interface definition\n\n# 2. RangeScan method exists\ngrep -n \"func (bt \\*BTree) RangeScan\" internal/storage/btree*.go\n# Expected: Shows method signature\n\n# 3. Uses sibling pointers\ngrep -n \"nextLeaf\" internal/storage/btree_iterator.go\n# Expected: Shows sibling pointer usage\n\n# 4. All tests pass\ngo test ./internal/storage/... -v -run TestRangeScan\n# Expected: All PASS\n\n# 5. No regression in existing tests\ngo test ./internal/storage/... -v\n# Expected: All PASS\n```\n\n## Definition of Done\n- [ ] `BTreeIterator` interface exists with `Next()`, `Key()`, `Value()`, `Err()`, `Close()`\n- [ ] `RangeScan(start, end []byte, opts ScanOptions) BTreeIterator` method on BTree\n- [ ] Iterator follows `nextLeaf` pointers (no tree re-traversal per key)\n- [ ] All 10 edge cases from table handled correctly\n- [ ] `TestRangeScanBasic` passes: correct keys in range\n- [ ] `TestRangeScanWithLimit` passes: stops at limit\n- [ ] `TestRangeScanEmptyRange` passes: no results for invalid range\n- [ ] Benchmark shows O(log N + K) behavior (not O(K log N))","status":"hooked","priority":2,"issue_type":"feature","assignee":"claudedb/polecats/capable","created_at":"2026-01-05T05:39:29.016147-07:00","created_by":"cabewaldrop","updated_at":"2026-01-07T14:20:07.105294-07:00","dependencies":[{"issue_id":"cdb-loe","depends_on_id":"cdb-a0f","type":"blocks","created_at":"2026-01-05T05:51:21.108895-07:00","created_by":"cabewaldrop"}]}
{"id":"cdb-lyg","title":"Use ScanWithFilter for early filtering","description":"The table.go already has ScanWithFilter() (line 322) but executor never uses it. Wire this up to reduce memory and enable early exit.\n\n## Current Behavior\n- executor.go lines 305-316 filter in memory AFTER loading all rows\n- All rows allocated even if only 1 matches\n\n## Implementation\n1. Instead of: rows := tbl.Scan(); filter(rows)\n2. Do: rows := tbl.ScanWithFilter(predicate)\n3. Filter while scanning, reduce allocations\n4. Can exit early when LIMIT is satisfied\n\n## Files\n- internal/sql/executor/executor.go\n- internal/table/table.go (ScanWithFilter already exists!)\n\n## Acceptance Criteria\n- SELECT with WHERE uses ScanWithFilter\n- Memory allocation reduced for selective queries\n- LIMIT queries can exit early","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-07T12:19:05.355156-07:00","created_by":"mayor","updated_at":"2026-01-07T14:05:19.905729-07:00"}
{"id":"cdb-m4l","title":"Table struct is not thread-safe, concurrent Inserts cause race conditions","description":"## Problem\n\nThe `Table` struct has no synchronization. Concurrent `Insert()` calls will cause race conditions leading to duplicate row IDs and data corruption.\n\n## Root Cause\n\n**table.go:181-192** - No mutex in struct:\n```go\ntype Table struct {\n    Name   string\n    Schema *Schema\n    pager        *storage.Pager\n    btree        *storage.BTree\n    nextRowID    uint64        // Not protected!\n    dataPageIDs  []uint32      // Not protected!\n    metadataPage uint32\n}\n```\n\n**table.go:269-270** - Non-atomic increment:\n```go\nrowID := t.nextRowID\nt.nextRowID++\n```\n\n## Race Condition Example\n\n```\nGoroutine A                 Goroutine B\n───────────────────────────────────────────\nread nextRowID (1)\n                            read nextRowID (1)  ← same value!\nnextRowID++ (now 2)\n                            nextRowID++ (now 2)\nreturn rowID=1\n                            return rowID=1  ← DUPLICATE!\n```\n\n## Impact\n\n- Duplicate row IDs\n- Data corruption\n- Potential panics from concurrent slice/map access\n\n## Suggested Fix\n\nAdd a mutex to protect all Table operations:\n\n```go\ntype Table struct {\n    mu sync.Mutex  // or sync.RWMutex for concurrent reads\n    // ... other fields\n}\n\nfunc (t *Table) Insert(values []Value) (uint64, error) {\n    t.mu.Lock()\n    defer t.mu.Unlock()\n    \n    rowID := t.nextRowID\n    t.nextRowID++\n    // ... rest of insert\n}\n\nfunc (t *Table) Scan() ([]Row, error) {\n    t.mu.Lock()  // or RLock() with RWMutex\n    defer t.mu.Unlock()\n    // ...\n}\n```\n\nUsing a mutex over atomics because Insert also modifies:\n- `dataPageIDs` (slice append)\n- `btree` (tree operations)\n\nThese all need protection, so a single mutex is cleaner than multiple atomics.\n\n## Files Affected\n\n- internal/table/table.go","status":"open","priority":2,"issue_type":"bug","created_at":"2026-01-04T06:13:27.675822-07:00","created_by":"cabewaldrop","updated_at":"2026-01-07T14:05:19.913059-07:00"}
{"id":"cdb-ok3","title":"Test prefix parsing","description":"Test","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-07T13:59:37.048287-07:00","created_by":"mayor","updated_at":"2026-01-07T14:05:19.908548-07:00","closed_at":"2026-01-07T13:59:43.871246-07:00","close_reason":"Closed"}
{"id":"cdb-xa7","title":"LRU buffer pool eviction","description":"Implement bounded buffer pool with LRU eviction to prevent unbounded memory growth.\n\n## Current State\n- pager.go line 40: cache is simple unbounded map\n- Memory grows with every page accessed\n- No eviction policy\n\n## Implementation\n1. Add maxCacheSize configuration\n2. Implement LRU tracking (doubly-linked list + map)\n3. Evict least recently used pages when cache full\n4. Track dirty pages for write-back before eviction\n\n## Files\n- internal/storage/pager.go (line ~40)\n\n## Acceptance Criteria\n- Cache has configurable size limit\n- LRU eviction when limit reached\n- Dirty pages written before eviction\n- Memory usage bounded regardless of table size","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-07T12:19:06.398096-07:00","created_by":"mayor","updated_at":"2026-01-07T14:05:19.913315-07:00"}
{"id":"cdb-xsr","title":"Support secondary indexes (CREATE INDEX)","description":"## Problem\nOnly primary keys are indexed. Queries on other columns always require full table scans.\n\n## Solution\nSupport CREATE INDEX / DROP INDEX for secondary indexes.\n\n## Syntax\n```sql\nCREATE INDEX idx_email ON users(email);\nCREATE UNIQUE INDEX idx_ssn ON users(ssn);\nDROP INDEX idx_email;\n```\n\n## Design\n\n### Index Metadata\n```go\ntype IndexInfo struct {\n    Name       string\n    TableName  string\n    ColumnName string\n    ColumnIdx  int\n    IsUnique   bool\n    RootPage   uint32\n}\n```\n\n### Catalog Changes\n```go\ntype Catalog struct {\n    // existing...\n    indexes map[string]map[string]*IndexInfo  // table -\u003e indexName -\u003e info\n}\n\nfunc (c *Catalog) CreateIndex(info *IndexInfo, btree *storage.BTree) error\nfunc (c *Catalog) DropIndex(tableName, indexName string) error\nfunc (c *Catalog) GetIndexes(tableName string) []*IndexInfo\nfunc (c *Catalog) GetIndex(tableName, indexName string) (*IndexInfo, bool)\n```\n\n### Table Changes\n```go\ntype Table struct {\n    // existing...\n    secondaryIndexes map[string]*storage.BTree\n    indexInfos       map[string]*IndexInfo\n}\n\nfunc (t *Table) AddIndex(info *IndexInfo, btree *storage.BTree)\nfunc (t *Table) RemoveIndex(indexName string)\nfunc (t *Table) GetIndexes() map[string]*IndexInfo\n```\n\n### Index Maintenance on DML\n\n```go\nfunc (t *Table) Insert(values []Value) (uint64, error) {\n    // Check unique constraints BEFORE insert\n    for name, idx := range t.secondaryIndexes {\n        info := t.indexInfos[name]\n        if info.IsUnique {\n            keyBytes := t.valueToBytes(values[info.ColumnIdx])\n            if _, found, _ := idx.Search(keyBytes); found {\n                return 0, fmt.Errorf(\n                    \"duplicate key value violates unique constraint \\\"%s\\\"\", name)\n            }\n        }\n    }\n    \n    // ... existing insert logic ...\n    \n    // Update all secondary indexes\n    for name, idx := range t.secondaryIndexes {\n        info := t.indexInfos[name]\n        keyBytes := t.valueToBytes(values[info.ColumnIdx])\n        if err := idx.Insert(keyBytes, location); err \\!= nil {\n            return 0, fmt.Errorf(\"failed to update index %s: %w\", name, err)\n        }\n    }\n    \n    return rowID, nil\n}\n\n// Similar for Update and Delete\n```\n\n### Executor Additions\n```go\nfunc (e *Executor) executeCreateIndex(stmt *CreateIndexStatement) (*Result, error) {\n    tbl := e.tables[stmt.TableName]\n    if tbl == nil {\n        return nil, fmt.Errorf(\"table %s does not exist\", stmt.TableName)\n    }\n    \n    colIdx, ok := tbl.Schema.GetColumnIndex(stmt.ColumnName)\n    if \\!ok {\n        return nil, fmt.Errorf(\"column %s does not exist\", stmt.ColumnName)\n    }\n    \n    // Check for duplicate index name\n    if _, exists := tbl.GetIndexes()[stmt.IndexName]; exists {\n        return nil, fmt.Errorf(\"index %s already exists\", stmt.IndexName)\n    }\n    \n    // Create B-tree for index\n    btree, err := storage.NewBTree(e.pager)\n    if err \\!= nil {\n        return nil, err\n    }\n    \n    // Populate index with existing data\n    rows, err := tbl.Scan()\n    if err \\!= nil {\n        return nil, err\n    }\n    \n    for _, row := range rows {\n        keyBytes := tbl.valueToBytes(row.Values[colIdx])\n        location := row.Location // Need to track this\n        \n        // Check unique constraint while building\n        if stmt.IsUnique {\n            if _, found, _ := btree.Search(keyBytes); found {\n                return nil, fmt.Errorf(\n                    \"could not create unique index: duplicate key found\")\n            }\n        }\n        \n        if err := btree.Insert(keyBytes, location); err \\!= nil {\n            return nil, err\n        }\n    }\n    \n    // Register index\n    info := \u0026IndexInfo{\n        Name:       stmt.IndexName,\n        TableName:  stmt.TableName,\n        ColumnName: stmt.ColumnName,\n        ColumnIdx:  colIdx,\n        IsUnique:   stmt.IsUnique,\n        RootPage:   btree.RootPage(),\n    }\n    \n    tbl.AddIndex(info, btree)\n    e.catalog.CreateIndex(info, btree)\n    \n    return \u0026Result{Message: fmt.Sprintf(\"Index '%s' created\", stmt.IndexName)}, nil\n}\n```\n\n## Files to Modify\n- internal/sql/lexer/lexer.go (INDEX, UNIQUE tokens)\n- internal/sql/parser/ast.go (CreateIndexStatement, DropIndexStatement)\n- internal/sql/parser/parser.go (parseCreateIndex, parseDropIndex)\n- internal/catalog/catalog.go (index storage)\n- internal/table/table.go (secondaryIndexes, index maintenance)\n- internal/sql/executor/executor.go (executeCreateIndex, executeDropIndex)\n- internal/sql/planner/planner.go (consider secondary indexes)\n\n## Edge Cases\n\n| Case | Scenario | Expected Behavior |\n|------|----------|-------------------|\n| Duplicate index name | CREATE INDEX foo twice | Error: \"index foo already exists\" |\n| Non-existent table | CREATE INDEX ON nosuchtable | Error: \"table nosuchtable does not exist\" |\n| Non-existent column | CREATE INDEX ON t(nocol) | Error: \"column nocol does not exist\" |\n| Drop non-existent | DROP INDEX nosuchindex | Error: \"index nosuchindex does not exist\" |\n| Unique violation on create | CREATE UNIQUE INDEX on column with duplicates | Error: \"duplicate key found\" |\n| Unique violation on insert | INSERT duplicate into unique-indexed column | Error: \"violates unique constraint\" |\n| NULL in unique index | INSERT NULL into unique column | Allowed (NULLs are distinct) |\n| Index on NULL-heavy column | Many NULL values | Index works, NULLs stored |\n| Concurrent index creation | CREATE INDEX during heavy INSERT | Acceptable race (may miss rows) |\n| Index survives restart | Create index, restart, query | Index metadata loaded from catalog |\n\n## Test Scenarios\n\n```go\nfunc TestCreateIndex(t *testing.T) {\n    exec := setupExecutorWithTable(t, \"users\",\n        \"id INTEGER PRIMARY KEY, email TEXT, age INTEGER\")\n    \n    // Insert test data\n    exec.Execute(parseSQL(\"INSERT INTO users VALUES (1, 'a@b.com', 30)\"))\n    exec.Execute(parseSQL(\"INSERT INTO users VALUES (2, 'c@d.com', 25)\"))\n    \n    // Create index\n    result, err := exec.Execute(parseSQL(\"CREATE INDEX idx_email ON users(email)\"))\n    require.NoError(t, err)\n    assert.Contains(t, result.Message, \"created\")\n    \n    // Verify index exists\n    tbl, _ := exec.GetTable(\"users\")\n    _, exists := tbl.GetIndexes()[\"idx_email\"]\n    assert.True(t, exists)\n}\n\nfunc TestUniqueIndexPreventssDuplicates(t *testing.T) {\n    exec := setupExecutorWithTable(t, \"users\",\n        \"id INTEGER PRIMARY KEY, email TEXT\")\n    \n    exec.Execute(parseSQL(\"CREATE UNIQUE INDEX idx_email ON users(email)\"))\n    exec.Execute(parseSQL(\"INSERT INTO users VALUES (1, 'a@b.com')\"))\n    \n    // Duplicate should fail\n    _, err := exec.Execute(parseSQL(\"INSERT INTO users VALUES (2, 'a@b.com')\"))\n    assert.ErrorContains(t, err, \"unique constraint\")\n}\n\nfunc TestDropIndex(t *testing.T) {\n    exec := setupExecutorWithTable(t, \"users\", \"id INTEGER PRIMARY KEY, name TEXT\")\n    exec.Execute(parseSQL(\"CREATE INDEX idx_name ON users(name)\"))\n    \n    result, err := exec.Execute(parseSQL(\"DROP INDEX idx_name\"))\n    require.NoError(t, err)\n    assert.Contains(t, result.Message, \"dropped\")\n    \n    // Index should no longer exist\n    tbl, _ := exec.GetTable(\"users\")\n    _, exists := tbl.GetIndexes()[\"idx_name\"]\n    assert.False(t, exists)\n}\n\nfunc TestIndexUsedByPlanner(t *testing.T) {\n    exec := setupExecutorWithTable(t, \"users\",\n        \"id INTEGER PRIMARY KEY, email TEXT\")\n    exec.Execute(parseSQL(\"CREATE INDEX idx_email ON users(email)\"))\n    \n    // Query on indexed column should use index\n    result, _ := exec.Execute(parseSQL(\n        \"EXPLAIN SELECT * FROM users WHERE email = 'test@test.com'\"))\n    \n    assert.Contains(t, result.Message, \"IndexScan\")\n    assert.Contains(t, result.Message, \"idx_email\")\n}\n\nfunc TestIndexMaintainedOnInsert(t *testing.T) {\n    exec := setupExecutorWithTable(t, \"users\",\n        \"id INTEGER PRIMARY KEY, email TEXT\")\n    exec.Execute(parseSQL(\"CREATE INDEX idx_email ON users(email)\"))\n    \n    // Insert after index creation\n    exec.Execute(parseSQL(\"INSERT INTO users VALUES (1, 'new@email.com')\"))\n    \n    // Query should find the new row via index\n    result, _ := exec.Execute(parseSQL(\n        \"SELECT * FROM users WHERE email = 'new@email.com'\"))\n    assert.Equal(t, 1, len(result.Rows))\n}\n\nfunc TestIndexSurvivesRestart(t *testing.T) {\n    dbPath := t.TempDir() + \"/test.db\"\n    \n    // Create, populate, close\n    exec1 := openDatabase(dbPath)\n    exec1.Execute(parseSQL(\"CREATE TABLE users (id INTEGER PRIMARY KEY, email TEXT)\"))\n    exec1.Execute(parseSQL(\"CREATE INDEX idx_email ON users(email)\"))\n    exec1.Execute(parseSQL(\"INSERT INTO users VALUES (1, 'test@test.com')\"))\n    exec1.Close()\n    \n    // Reopen and verify\n    exec2 := openDatabase(dbPath)\n    result, err := exec2.Execute(parseSQL(\n        \"EXPLAIN SELECT * FROM users WHERE email = 'test@test.com'\"))\n    require.NoError(t, err)\n    assert.Contains(t, result.Message, \"idx_email\")\n    exec2.Close()\n}\n```\n\n## Agent Verification Checklist\n\n```bash\n# 1. CREATE INDEX parses\necho \"CREATE INDEX idx ON t(col)\" | go run ./cmd/claude-db/ 2\u003e\u00261 | grep -v \"error\"\n# Expected: No parse error\n\n# 2. IndexInfo struct exists  \ngrep -n \"type IndexInfo struct\" internal/catalog/catalog.go\n# Expected: Shows struct definition\n\n# 3. Index maintenance in Insert\ngrep -n \"secondaryIndexes\" internal/table/table.go | head -5\n# Expected: Shows secondary index handling\n\n# 4. Planner considers secondary indexes\ngrep -n \"GetIndexes\" internal/sql/planner/planner.go\n# Expected: Shows planner checking for indexes\n\n# 5. All tests pass\ngo test ./... -v -run \"TestCreateIndex\\|TestUniqueIndex\\|TestDropIndex\\|TestIndexUsed\\|TestIndexMaintained\\|TestIndexSurvives\"\n# Expected: All PASS\n```\n\n## Definition of Done\n- [ ] `CREATE INDEX name ON table(column)` parses and executes\n- [ ] `CREATE UNIQUE INDEX` enforces uniqueness\n- [ ] `DROP INDEX name` removes index\n- [ ] `IndexInfo` stored in catalog, survives restart\n- [ ] `Table.Insert()` updates all secondary indexes\n- [ ] `Table.Delete()` removes from all secondary indexes  \n- [ ] `Table.Update()` updates indexes when indexed column changes\n- [ ] Query planner considers secondary indexes for WHERE clauses\n- [ ] `TestCreateIndex` passes: index created and registered\n- [ ] `TestUniqueIndexPreventssDuplicates` passes: unique constraint works\n- [ ] `TestDropIndex` passes: index removed\n- [ ] `TestIndexUsedByPlanner` passes: EXPLAIN shows index usage\n- [ ] `TestIndexMaintainedOnInsert` passes: new rows findable via index\n- [ ] `TestIndexSurvivesRestart` passes: persistence works","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-05T05:39:34.262975-07:00","created_by":"cabewaldrop","updated_at":"2026-01-07T14:05:19.915552-07:00","dependencies":[{"issue_id":"cdb-xsr","depends_on_id":"cdb-goh","type":"blocks","created_at":"2026-01-05T05:40:24.34279-07:00","created_by":"cabewaldrop"},{"issue_id":"cdb-xsr","depends_on_id":"cdb-loe","type":"blocks","created_at":"2026-01-05T05:40:29.579778-07:00","created_by":"cabewaldrop"},{"issue_id":"cdb-xsr","depends_on_id":"cdb-au8","type":"blocks","created_at":"2026-01-05T05:51:26.363511-07:00","created_by":"cabewaldrop"}]}
{"id":"claude-db-test-xyz","title":"Test force","description":"Test","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-07T14:04:23.235005-07:00","created_by":"mayor","updated_at":"2026-01-07T14:04:47.014839-07:00","close_reason":"Closed","deleted_at":"2026-01-07T14:04:47.014839-07:00","deleted_by":"batch delete","delete_reason":"batch delete","original_type":"task"}
